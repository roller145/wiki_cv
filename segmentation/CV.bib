
@inproceedings{shotton_textonboost:_2006,
	title = {{TextonBoost}: Joint Appearance, Shape and Context Modeling for Multi-class Object Recognition and Segmentation},
	isbn = {978-3-540-33833-8},
	series = {Lecture Notes in Computer Science},
	shorttitle = {{TextonBoost}},
	abstract = {This paper proposes a new approach to learning a discriminative model of object classes, incorporating appearance, shape and context information efficiently. The learned model is used for automatic visual recognition and semantic segmentation of photographs. Our discriminative model exploits novel features, based on textons, which jointly model shape and texture. Unary classification and feature selection is achieved using shared boosting to give an efficient classifier which can be applied to a large number of classes. Accurate image segmentation is achieved by incorporating these classifiers in a conditional random field. Efficient training of the model on very large datasets is achieved by exploiting both random feature selection and piecewise training methods.High classification and segmentation accuracy are demonstrated on three different databases: i) our own 21-object class database of photographs of real objects viewed under general lighting conditions, poses and viewpoints, ii) the 7-class Corel subset and iii) the 7-class Sowerby database used in [1]. The proposed algorithm gives competitive results both for highly textured (e.g. grass, trees), highly structured (e.g. cars, faces, bikes, aeroplanes) and articulated objects (e.g. body, cow).},
	booktitle = {Computer Vision – {ECCV} 2006},
	author = {Shotton, Jamie and Winn, John and Rother, Carsten and Criminisi, Antonio},
	date = {2006},
	langid = {english},
	keywords = {Class Label, Conditional Random Field, Context Modeling, Object Class, Training Image}
}

@article{badrinarayanan_segnet:_2015,
	title = {{SegNet}: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation},
	url = {http://arxiv.org/abs/1511.00561},
	shorttitle = {{SegNet}},
	abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed {SegNet}. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the {VGG}16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of {SegNet} lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted {FCN} and also with the well known {DeepLab}-{LargeFOV}, {DeconvNet} architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. {SegNet} was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures. We also performed a controlled benchmark of {SegNet} and other architectures on both road scenes and {SUN} {RGB}-D indoor scene segmentation tasks. We show that {SegNet} provides good performance with competitive inference time and more efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of {SegNet} and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
	journaltitle = {{arXiv}:1511.00561 [cs]},
	author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
	urldate = {2018-12-18},
	date = {2015-11-02},
	eprinttype = {arxiv},
	eprint = {1511.00561},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1511.00561 PDF:/Users/irina-rud/Zotero/storage/3EZGNWZA/Badrinarayanan и др. - 2015 - SegNet A Deep Convolutional Encoder-Decoder Archi.pdf:application/pdf;arXiv.org Snapshot:/Users/irina-rud/Zotero/storage/Z9HXH7QQ/1511.html:text/html}
}

@inproceedings{long_fully_2015,
	title = {Fully Convolutional Networks for Semantic Segmentation},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html},
	eventtitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {3431--3440},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	urldate = {2018-12-18},
	date = {2015},
	file = {Full Text PDF:/Users/irina-rud/Zotero/storage/ENMTQSTI/Long и др. - 2015 - Fully Convolutional Networks for Semantic Segmenta.pdf:application/pdf;Snapshot:/Users/irina-rud/Zotero/storage/NNMHDWSF/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html:text/html}
}

@article{chen_deeplab:_2018,
	title = {{DeepLab}: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected {CRFs}},
	volume = {40},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2017.2699184},
	shorttitle = {{DeepLab}},
	abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or `atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling ({ASPP}) to robustly segment objects at multiple scales. {ASPP} probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from {DCNNs} and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in {DCNNs} achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final {DCNN} layer with a fully connected Conditional Random Field ({CRF}), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed “{DeepLab}” system sets the new state-of-art at the {PASCAL} {VOC}-2012 semantic image segmentation task, reaching 79.7 percent {mIOU} in the test set, and advances the results on three other datasets: {PASCAL}-Context, {PASCAL}-Person-Part, and Cityscapes. All of our code is made publicly available online.},
	pages = {834--848},
	number = {4},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Chen, L. and Papandreou, G. and Kokkinos, I. and Murphy, K. and Yuille, A. L.},
	date = {2018-04},
	keywords = {atrous convolution, atrous spatial pyramid pooling, Computational modeling, conditional random fields, Context, convolution, Convolution, Convolutional neural networks, deep convolutional nets, Deep Convolutional Neural Networks, Deep Learning, {DeepLab}, feature extraction, feedforward neural nets, fully connected Conditional Random Field, highlight convolution, image context, Image resolution, image segmentation, Image segmentation, learning (artificial intelligence), Neural networks, {PASCAL} {VOC}-2012 semantic image segmentation task, probabilistic graphical models, random processes, semantic image segmentation, semantic segmentation, Semantics},
	file = {IEEE Xplore Abstract Record:/Users/irina-rud/Zotero/storage/RAAHHP3Z/7913730.html:text/html}
}